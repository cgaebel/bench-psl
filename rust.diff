diff --git a/src/libstd/collections/hash/map.rs b/src/libstd/collections/hash/map.rs
index 80ae307..c898f7b 100644
--- a/src/libstd/collections/hash/map.rs
+++ b/src/libstd/collections/hash/map.rs
@@ -45,19 +45,18 @@ use super::table::BucketState::{
 };
 use super::state::HashState;

-const INITIAL_LOG2_CAP: uint = 5;
-pub const INITIAL_CAPACITY: uint = 1 << INITIAL_LOG2_CAP; // 2^5
+const INITIAL_LOG2_CAP: uint = 4;
+pub const INITIAL_CAPACITY: uint = 1 << INITIAL_LOG2_CAP; // 2^4

-/// The default behavior of HashMap implements a load factor of 90.9%.
-/// This behavior is characterized by the following condition:
-///
-/// - if size > 0.909 * capacity: grow the map
 #[derive(Clone)]
-struct DefaultResizePolicy;
+struct DefaultResizePolicy {
+    load_factor: f64,
+}

 impl DefaultResizePolicy {
-    fn new() -> DefaultResizePolicy {
-        DefaultResizePolicy
+    fn new(load_factor: f64) -> DefaultResizePolicy {
+        assert!(load_factor <= 1.);
+        DefaultResizePolicy { load_factor: load_factor }
     }

     #[inline]
@@ -66,7 +65,7 @@ impl DefaultResizePolicy {
         // on capacity:
         //
         // - if `cap < size * 1.1`: grow the map
-        usable_size * 11 / 10
+        (usable_size as f64 * (1. / self.load_factor)) as uint
     }

     /// An inverse of `min_capacity`, approximately.
@@ -81,14 +80,14 @@ impl DefaultResizePolicy {
         //
         // This doesn't have to be checked for overflow since allocation size
         // in bytes will overflow earlier than multiplication by 10.
-        cap * 10 / 11
+        (cap as f64 * self.load_factor) as uint
     }
 }

 #[test]
 fn test_resize_policy() {
     use prelude::v1::*;
-    let rp = DefaultResizePolicy;
+    let rp = DefaultResizePolicy { load_factor: 0.9 };
     for n in range(0u, 1000) {
         assert!(rp.min_capacity(rp.usable_capacity(n)) <= n);
         assert!(rp.usable_capacity(rp.min_capacity(n)) <= n);
@@ -519,6 +518,11 @@ impl<K: Hash<Hasher> + Eq, V> HashMap<K, V, RandomState> {
     }
 }

+#[test]
+fn init_capacity_next_pow_2() {
+    assert_eq!(INITIAL_CAPACITY, INITIAL_CAPACITY.next_power_of_two());
+}
+
 impl<K, V, S, H> HashMap<K, V, S>
     where K: Eq + Hash<H>,
           S: HashState<Hasher=H>,
@@ -543,7 +547,7 @@ impl<K, V, S, H> HashMap<K, V, S>
     pub fn with_hash_state(hash_state: S) -> HashMap<K, V, S> {
         HashMap {
             hash_state:    hash_state,
-            resize_policy: DefaultResizePolicy::new(),
+            resize_policy: DefaultResizePolicy::new(0.9),
             table:         RawTable::new(0),
         }
     }
@@ -570,17 +574,31 @@ impl<K, V, S, H> HashMap<K, V, S>
     #[unstable = "hasher stuff is unclear"]
     pub fn with_capacity_and_hash_state(capacity: uint, hash_state: S)
                                         -> HashMap<K, V, S> {
-        let resize_policy = DefaultResizePolicy::new();
+        HashMap::with_capacity_hash_state_and_load_factor(capacity, hash_state, 0.9)
+    }
+
+    /// Missing docs.
+    #[inline]
+    pub fn with_capacity_hash_state_and_load_factor(
+        capacity: uint, hash_state: S, load_factor: f64) -> HashMap<K, V, S> {
+        let resize_policy = DefaultResizePolicy::new(load_factor);
         let min_cap = max(INITIAL_CAPACITY, resize_policy.min_capacity(capacity));
         let internal_cap = min_cap.checked_next_power_of_two().expect("capacity overflow");
-        assert!(internal_cap >= capacity, "capacity overflow");
+        assert!(internal_cap >= capacity, "capacity_overflow");
         HashMap {
-            hash_state:    hash_state,
+            hash_state: hash_state,
             resize_policy: resize_policy,
-            table:         RawTable::new(internal_cap),
+            table: RawTable::new(internal_cap),
         }
     }

+    /// Returns the average and max probe sequence lengths. Use this to tune the
+    /// load factor.
+    #[inline]
+    pub fn avg_max_psl(&self) -> (f64, f64) {
+        self.table.avg_max_psl()
+    }
+
     /// Returns the number of elements the map can hold without reallocating.
     ///
     /// # Example
diff --git a/src/libstd/collections/hash/table.rs b/src/libstd/collections/hash/table.rs
index f28b95d..0244dfe 100644
--- a/src/libstd/collections/hash/table.rs
+++ b/src/libstd/collections/hash/table.rs
@@ -15,7 +15,7 @@ use self::BucketState::*;
 use clone::Clone;
 use cmp;
 use hash::{Hash, Hasher};
-use iter::{Iterator, ExactSizeIterator, count};
+use iter::{range, Iterator, ExactSizeIterator, count};
 use marker::{Copy, Sized, self};
 use mem::{min_align_of, size_of};
 use mem;
@@ -630,6 +630,26 @@ impl<K, V> RawTable<K, V> {
         }
     }

+    pub fn avg_max_psl(&self) -> (f64, f64) {
+        let cap = self.capacity;
+
+        let mut tot_psl = 0;
+        let mut max_psl = 0;
+
+        for i in range(0, cap) {
+            let hash = unsafe { *self.hashes.0.offset(i as int) };
+
+            if hash == 0 { continue; }
+
+            let psl = (i - (hash as uint)) & (cap - 1);
+
+            tot_psl += psl;
+            max_psl = cmp::max(max_psl, psl);
+        }
+
+        (tot_psl as f64 / self.size as f64, max_psl as f64)
+    }
+
     /// Creates a new raw table from a given capacity. All buckets are
     /// initially empty.
     #[allow(unstable)]
